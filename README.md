# Comparison of DDPG and PPO Algorithms

## üìã –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞
–í —ç—Ç–æ–º –ø—Ä–æ–µ–∫—Ç–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–≤—É—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º: Deep Deterministic Policy Gradient (DDPG) –∏ Proximal Policy Optimization (PPO). –¶–µ–ª—å ‚Äî –æ—Ü–µ–Ω–∏—Ç—å –∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö.

## üõ†Ô∏è –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫
- **–Ø–∑—ã–∫:** Python  
- **–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏:** TensorFlow, PyTorch, OpenAI Gym, NumPy, Matplotlib 
- **–°—Ä–µ–¥–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏:** Jupyter Notebook 

## ‚ö° –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
- –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ DDPG –∏ PPO.
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç–ø–∏–∑–æ–¥–∞—Ö.
- –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–±–æ–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤. 
